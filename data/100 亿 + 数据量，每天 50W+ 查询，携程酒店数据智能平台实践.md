原文链接：[100 亿 + 数据量，每天 50W+ 查询，携程酒店数据智能平台实践](https://www.infoq.cn/article/MHXgE54xXTYECOjqEFLH)

原文链接：100 亿 + 数据量，每天 50W+ 查询，携程酒店数据智能平台实践

问题存在的原因：

- 散：数据分散在不同平台，没有地方可以统一查看所有数据；
- 杂：不同平台逻辑不同，没有统一评判标准；
- 浅：数据明细不够直观深入，无法清楚地了解趋势及问题；
- 慢：查询速度慢，临时取数流程漫长；
- 晚：当时存在的数据报表平台都无法实现实时的数据监控，对于业务在工作中，特别是订单高峰期库存时刻在变化的时候，不能起到很好的指导和推动作用；

方案选型：

1）ClickHouse 查询速度快，但无法承受高并发；

2）ElasticSearch 查询速度快，cpu 消耗到 60% 对查询性能不会有太大的影响，但不能做多表 join，大宽表维护成本不现实，约束了我们的使用场景；

3）Ingite 虽然也是内存数据库，但性能在高并发的时候内存会打爆，不能满足我们的要求，这个是用 5 台 24G 内存的虚拟机测试结果；

4）Presto 查询时直接读取 hive 表，能减少数据同步的流程，降低开发成本，查询速度勉强能接受，但不能满足高可用。因此这个只针对我们团队内部应用场景，不对业务端需求采用该技术方案；

5）CrateDB 底层沿用了 ElasticSearch 的源码，支持 SQL 语法，比 ElasticSearch 的使用更友好，也解决了 es 不能 join 的问题，但在多表 join 的场景 qps 达到 20 个左右内存就会被打爆（6 台 8 核 24G 内存虚拟机测试场景），单表查询性能和高并发支撑还是可以的；

6）MongoDB 走索引查询速度非常快，但太依赖左侧原则，也不能 join，只能通过嵌套文档的方案解决 join 的问题，但我们的查询条件太多，不能强依赖左侧原则来查询；

方案落地

实践中我们总结了以下方式来优化接口性能同时起到保护 ClickHouse 的作用：

1）做好查询监控，针对每个接口，每个查询语句都能知道查询消耗时间，case by case 优化；

2）所有数据查询请求拆成异步请求，避免大接口中数据请求等待的过程影响数据展示的速度；

3）针对使用率很高数据量又非常大的表，可以创建一个全量表，同时也创建一个只有最近 6 个月数据的表。因为我们通过埋点发现，90% 以上的查询都主要集中在查最近 6 个月的数据。所以有 90% 以上的查询使用的表数据量远远小于全量表，性能会好很多，服务器的开销也会小很多。当然这个方案也是需要 case by case 看的，也许某些 case 用户访问量最高的数据集中在最近 3 个月，可以根据实际情况来定；

4）统计出日常调用量比较大的接口，有针对性的做如下优化：

固定缓存：如果数据固定范围或者对于访问量高的页面默认查询条件，数据当天更新后由 job 触发主动模拟用户查询，提前为用户把数据缓存到 redis 中。用户在上班时间段查询就会从 redis 中拿数据，性能提高了，ClickHouse 的压力也降低了，也避免了用户高峰期间集中查询对 ClickHouse 服务器的冲击。

动态缓存：如果数据范围不固定，但调用量也很大，特别是实时数据，为了 ClickHouse 的稳定性，也建议增加缓存。我们曾经在这里踩过坑，用户会用同样的条件不断的刷数据，也许他在期待业绩数据的变化，遇到高并发的时候会把 ClickHouse 服务器 CPU 打满。实际上通过埋点发现，哪怕缓存时间只有 3 分钟也可以降低 50% 以上的 ClickHouse 查询请求。

分流机制：ClickHouse 主要是解决我们大表 join 查询性能，实际应用中可以将一些场景拆解，比如一些一线业务的权限比较小，对应权限的酒店数据量也会比较小。我们可以定义一个阀值，比如小于 5000 或者 8000 的数据走 mysql，这部分人走 mysql 速度也会很快，让权限大的用户走 ClickHouse，这样会引流很大一部分用户，提升整个平台的查询性能
